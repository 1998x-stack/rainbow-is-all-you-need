{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Reference: [OpenAI spinning-up](https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py#L10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple FIFO experience replay buffer for SAC agents.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, act_dim: int, size: int, batch_size: int = 32):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size, self.batch_size = 0, 0, size, batch_size\n",
    "\n",
    "    def store(self, obs: np.ndarray, act: int, rew: float,\n",
    "              next_obs: np.ndarray, done: bool):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        memory (PrioritizedReplayBuffer): replay memory\n",
    "        dqn (nn.Module): actor model to select actions\n",
    "        dqn_target (nn.Module): target actor model to select actions\n",
    "        dqn_optimizer (Optimizer): optimizer for training actor\n",
    "        epsilon (float): parameter for epsilon greedy policy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, memory: ReplayBuffer, update_period: int, \n",
    "                 epsilon: float = 1.0, gamma: float = 0.99, lr: float = 1e-4):\n",
    "        \"\"\"Initialization.\n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.memory = memory\n",
    "        self.epsilon = epsilon\n",
    "        self.update_period = update_period\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network().to(device)\n",
    "        self.dqn_target = Network().to(device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=LR)\n",
    "\n",
    "        # counter\n",
    "        self.cnt_update = 0\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input space.\"\"\"\n",
    "        # epsilon greedy policy\n",
    "        if not self.args.test and self.epsilon > np.random.random():\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            selected_action = self.dqn(state).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Return dqn loss.\"\"\"\n",
    "        state = torch.FloatTensor(samples[\"obs1\"])\n",
    "        next_state = torch.FloatTensor(samples[\"obs2\"])\n",
    "        action = samples[\"act\"]\n",
    "        reward = samples[\"rews\"]\n",
    "        done = samples[\"done\"]\n",
    "\n",
    "        q_value = self.dqn(states)\n",
    "        next_q_value = self.dqn_target(next_states)\n",
    "\n",
    "        curr_q_value = q_value.gather(1, action.unsqueeze(1))\n",
    "        next_q_value = next_q_value.gather(1, next_q_values.argmax(1).unsqueeze(1))\n",
    "\n",
    "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
    "        #       = r                       otherwise\n",
    "        masks = 1 - dones\n",
    "        target = rewards + self.gamma * next_q_value * masks\n",
    "        target = target.to(device)\n",
    "\n",
    "        # calculate dq loss\n",
    "        dq_loss = F.smooth_l1_loss(curr_q_value, target.detach())\n",
    "\n",
    "        return dq_loss\n",
    "\n",
    "    def update_model(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Train the model after each episode.\"\"\"\n",
    "        self.cnt_update += 1\n",
    "        samples = self.memory.sample_batch(self.beta)\n",
    "\n",
    "        dq_loss = self.compute_dqn_loss(samples)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target networks\n",
    "        if self.cnt_update % self.update_period == 0:\n",
    "            self.update_target_n\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        \"\"\"Update target network's weights.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 10000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
